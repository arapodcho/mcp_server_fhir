import asyncio
import os
import sys
from typing import Annotated, Literal, List
from typing_extensions import TypedDict

# LangChain / LangGraph Imports
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, ToolMessage, BaseMessage, AIMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver # ëŒ€í™” ê¸°ì–µìš©

# MCP Imports (User's custom client & Standard types)
from mcp.types import CallToolResult
# from your_module import MultiServerMCPClient  <-- ì‚¬ìš©ìì˜ í´ë˜ìŠ¤ import í•„ìš”

from dotenv import load_dotenv
load_dotenv()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
GOOGLE_MODEL_NAME = os.getenv("GOOGLE_MODEL_NAME", "gemini-2.5-flash")
MCP_TRANSPORT_METHOD = os.getenv("MCP_TRANSPORT_METHOD", "sse")  # 'sse' or 'stdio'

MCP_NAME = os.getenv("MCP_NAME", "fhir-mcp")
MCP_IP = os.getenv("MCP_IP", "0.0.0.0")
MCP_PORT = int(os.getenv("MCP_PORT", "8052"))
mcp_set_dict = {}
if MCP_TRANSPORT_METHOD != 'stdio':
    mcp_connection = f"http://{MCP_IP}:{MCP_PORT}/{MCP_TRANSPORT_METHOD}"

    mcp_set_dict = {
            str(MCP_NAME): {
                "url": str(mcp_connection),
                "transport": str(MCP_TRANSPORT_METHOD),
            }
        }
else:
    # stdio ëª¨ë“œì¸ ê²½ìš°, ë³„ë„ ì„¤ì • ì—†ì´ subprocessì—ì„œ ìë™ ì—°ê²°ë¨
    mcp_set_dict = {
            str(MCP_NAME): {
                "transport": "stdio",
                "command": sys.executable,
                "args": [os.path.join(os.path.dirname(__file__), "fastmcp_server.py")],
            }
        }
# =============================================================================
# 1. Helper Functions & State Definition
# =============================================================================

class AgentState(TypedDict):
    # add_messages: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê³„ì† ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì (Append)í•˜ëŠ” Reducer
    messages: Annotated[List[BaseMessage], add_messages]

def mcp_tools_to_schema(mcp_list_tools_result):
    """MCP Tool ì •ì˜ë¥¼ Geminiê°€ ì´í•´í•˜ëŠ” JSON Schemaë¡œ ë³€í™˜"""
    tools_schema = []
    for tool in mcp_list_tools_result.tools:
        tools_schema.append({
            "name": tool.name,
            "description": tool.description,
            "parameters": tool.inputSchema
        })
    return tools_schema

# =============================================================================
# 2. Main Chat Application
# =============================================================================

async def run_chat_app():
    # 1. MCP Client ì„¤ì • (ì‚¬ìš©ì ì œê³µ ì½”ë“œ)
    # ì‹¤ì œë¡œëŠ” MultiServerMCPClient í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” import í–ˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ê¸°ì¡´ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    from langchain_mcp_adapters.client import MultiServerMCPClient # (ì˜ˆì‹œ) íŒŒì¼ ë¶„ë¦¬ ê¶Œì¥

    client = MultiServerMCPClient(mcp_set_dict)
    print("ğŸ”Œ Connecting to MCP Server...")

    # â˜… í•µì‹¬: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œ ì±—ë´‡ ë£¨í”„ë¥¼ ì‹¤í–‰í•´ì•¼ í•¨ â˜…
    async with client.session(MCP_NAME) as mcp:
        
        # 2. ë„êµ¬ ë¡œë“œ ë° LLM ì„¤ì •
        try:
            mcp_tools = await mcp.list_tools()
            formatted_tools = mcp_tools_to_schema(mcp_tools)
            print(f"ğŸ› ï¸  Loaded {len(formatted_tools)} tools from MCP Server.")
        except Exception as e:
            print(f"âŒ Error loading tools: {e}")
            return

        # Gemini ëª¨ë¸ ì´ˆê¸°í™”
        llm = ChatGoogleGenerativeAI(
            model=GOOGLE_MODEL_NAME,
            temperature=0,
            google_api_key=GOOGLE_API_KEY
        )
        llm_with_tools = llm.bind_tools(formatted_tools)

        # ---------------------------------------------------------------------
        # 3. Graph Nodes Definition (ë‚´ë¶€ í•¨ìˆ˜ë¡œ ì •ì˜í•˜ì—¬ 'mcp' ë³€ìˆ˜ ì ‘ê·¼)
        # ---------------------------------------------------------------------
        
        # [Node 1] ì±—ë´‡(Agent) ë…¸ë“œ
        def chatbot_node(state: AgentState):
            return {"messages": [llm_with_tools.invoke(state["messages"])]}

        # [Node 2] ë„êµ¬ ì‹¤í–‰(Tool) ë…¸ë“œ
        async def tool_node(state: AgentState):
            last_message = state["messages"][-1]
            tool_results = []

            for tool_call in last_message.tool_calls:
                print(f"\nâš™ï¸  [Tool Call] {tool_call['name']} (Args: {tool_call['args']})")
                
                try:
                    # MCP ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë„êµ¬ í˜¸ì¶œ
                    result: CallToolResult = await mcp.call_tool(
                        name=tool_call["name"],
                        arguments=tool_call["args"]
                    )
                    
                    # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    content = result.content[0].text if result.content else "No content returned."
                    print(f"   âœ… Result: {content[:100]}..." if len(content) > 100 else f"   âœ… Result: {content}")

                except Exception as e:
                    content = f"Error executing tool: {str(e)}"
                    print(f"   âŒ Error: {content}")

                # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
                tool_results.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    name=tool_call["name"],
                    content=str(content)
                ))
            
            return {"messages": tool_results}

        # [Edge] ì¡°ê±´ë¶€ ë¶„ê¸°
        def should_continue(state: AgentState) -> Literal["tools", "__end__"]:
            last_message = state["messages"][-1]
            if last_message.tool_calls:
                return "tools"
            return "__end__"

        # ---------------------------------------------------------------------
        # 4. Graph Construction
        # ---------------------------------------------------------------------
        workflow = StateGraph(AgentState)
        workflow.add_node("agent", chatbot_node)
        workflow.add_node("tools", tool_node)

        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges("agent", should_continue)
        workflow.add_edge("tools", "agent")

        # MemorySaver: ëŒ€í™” ë‚´ì—­ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (Multi-turn í•µì‹¬)
        checkpointer = MemorySaver()
        app = workflow.compile(checkpointer=checkpointer)

        # ---------------------------------------------------------------------
        # 5. Interactive Chat Loop (Multi-turn)
        # ---------------------------------------------------------------------
        thread_id = "session-1" # ì‚¬ìš©ì ì„¸ì…˜ ID
        config = {"configurable": {"thread_id": thread_id}}
        
        print("\n" + "="*50)
        print("ğŸ¤– Clinical AI Chatbot is Ready! (type 'exit' to quit)")
        print("="*50)

        while True:
            try:
                user_input = input("\nğŸ‘¤ User: ")
                if user_input.lower() in ["exit", "quit", "ê·¸ë§Œ"]:
                    print("ğŸ‘‹ Chat session ended.")
                    break
                
                # ê·¸ë˜í”„ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)
                # ì´ì „ ëŒ€í™” ê¸°ë¡ì€ checkpointerê°€ ê´€ë¦¬í•˜ë¯€ë¡œ ìƒˆë¡œìš´ ì…ë ¥ë§Œ ë„£ìœ¼ë©´ ë¨
                async for event in app.astream(
                    {"messages": [HumanMessage(content=user_input)]}, 
                    config=config
                ):
                    for key, value in event.items():
                        if key == "agent":
                            msg = value["messages"][-1]
                            if msg.content:
                                content = msg.content
                                if isinstance(content, list):
                                    text_parts = [part.get("text", "") for part in content if isinstance(part, dict)]
                                    print(f"ğŸ¤– AI: {''.join(text_parts) if text_parts else content}")
                                elif isinstance(content, dict):
                                    print(f"ğŸ¤– AI: {content.get('text', content)}")
                                else:
                                    print(f"ğŸ¤– AI: {content}")
                        # tool ì¶œë ¥ì€ ìœ„ nodeì—ì„œ print ì°ìŒ
            
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Forced exit.")
                break
            except Exception as e:
                print(f"âŒ System Error: {e}")

if __name__ == "__main__":
    # Windows í™˜ê²½ asyncio ì •ì±… ì„¤ì • (í•„ìš”ì‹œ)
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
    asyncio.run(run_chat_app())
    